{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d795ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f3ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"../files/models/model.pkl.gz\"\n",
    "METRICS_FILENAME = \"../files/output/metrics.json\"\n",
    "TRAIN_PATH = os.path.join(\"..\", \"files\", \"input\", \"train_data.csv.zip\")\n",
    "TEST_PATH = os.path.join(\"..\", \"files\", \"input\", \"test_data.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef87584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGA\n",
    "def load_raw_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Carga los CSV comprimidos de entrenamiento y prueba.\"\"\"\n",
    "    if not os.path.exists(TRAIN_PATH):\n",
    "        raise FileNotFoundError(f\"No encuentro {TRAIN_PATH}\")\n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        raise FileNotFoundError(f\"No encuentro {TEST_PATH}\")\n",
    "\n",
    "    df_train = pd.read_csv(TRAIN_PATH)\n",
    "    df_test = pd.read_csv(TEST_PATH)\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c196e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LIMPIEZA\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica las transformaciones del Paso 1:\n",
    "    - Renombra \"default payment next month\" a \"default\".\n",
    "    - Elimina la columna \"ID\".\n",
    "    - Agrupa EDUCATION > 4 en 4 (\"others\").\n",
    "    - Elimina filas con NA.\n",
    "    \"\"\"\n",
    "    # Renombrar variable objetivo\n",
    "    if \"default payment next month\" in df.columns:\n",
    "        df = df.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "    # Quitar columna ID si existe\n",
    "    if \"ID\" in df.columns:\n",
    "        df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # EDUCATION > 4 -> 4\n",
    "    if \"EDUCATION\" in df.columns:\n",
    "        df.loc[df[\"EDUCATION\"] > 4, \"EDUCATION\"] = 4\n",
    "        df = df.query('MARRIAGE != 0 and EDUCATION != 0')\n",
    "\n",
    "    # Eliminar filas con NA\n",
    "    df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9717aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Separa X y (Paso 2).\"\"\"\n",
    "    if \"default\" not in df.columns:\n",
    "        raise RuntimeError('La columna objetivo \"default\" no existe en el dataset.')\n",
    "\n",
    "    y = df[\"default\"].astype(int)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5816b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PIPELINE + GRID SEARCH\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(x_train: pd.DataFrame) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Crea el pipeline y el GridSearchCV (Pasos 3 y 4).\n",
    "\n",
    "    Pipeline:\n",
    "      - ColumnTransformer con OneHotEncoder para columnas categóricas\n",
    "        y MinMaxScaler para columnas numéricas\n",
    "      - SelectKBest para seleccionar las K mejores\n",
    "      - LogisticRegression como modelo final\n",
    "    \"\"\"\n",
    "\n",
    "    # Columnas categóricas\n",
    "    categorical_features: List[str] = []\n",
    "    for col in [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]:\n",
    "        if col in x_train.columns:\n",
    "            categorical_features.append(col)\n",
    "\n",
    "    # El resto se considera numérico\n",
    "    numeric_features = [c for c in x_train.columns if c not in categorical_features]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"cat\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                categorical_features,\n",
    "            ),\n",
    "            (\n",
    "                \"num\",\n",
    "                MinMaxScaler(),\n",
    "                numeric_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif)\n",
    "\n",
    "    # Regresión logística estándar (sin class_weight)\n",
    "    logreg = LogisticRegression(\n",
    "        max_iter=5000,\n",
    "        solver=\"lbfgs\",    # muy común en ejemplos de clase\n",
    "        n_jobs=-1,\n",
    "        class_weight=None,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"select\", selector),\n",
    "            (\"logreg\", logreg),\n",
    "        ],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Rejilla de hiperparámetros: K y C\n",
    "    # Puedes ajustar estos valores si ves que no alcanza\n",
    "    k_values = [1, 10, 13]\n",
    "\n",
    "    param_grid = {\n",
    "        \"select__k\": k_values,\n",
    "        \"logreg__C\": [0.1, 1.0, 10.0, 100.0],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=10,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56dde0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_classification_metrics(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Diccionario con las métricas que pide el test (Paso 6).\"\"\"\n",
    "    return {\n",
    "        \"type\": \"metrics\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"balanced_accuracy\": float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "137707ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix_dict(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Diccionario con la matriz de confusión en el formato del Paso 7.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"true_0\": {\"predicted_0\": int(tn), \"predicted_1\": int(fp)},\n",
    "        \"true_1\": {\"predicted_0\": int(fn), \"predicted_1\": int(tp)},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eaaae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GUARDAR MODELO Y MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def save_model(model) -> None:\n",
    "    os.makedirs(os.path.dirname(MODEL_FILENAME), exist_ok=True)\n",
    "    with gzip.open(MODEL_FILENAME, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def save_metrics(records) -> None:\n",
    "    \"\"\"\n",
    "    records: lista de diccionarios.\n",
    "    Orden esperado por el test:\n",
    "      0 -> métricas train\n",
    "      1 -> métricas test\n",
    "      2 -> matriz confusión train\n",
    "      3 -> matriz confusión test\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(METRICS_FILENAME), exist_ok=True)\n",
    "    with open(METRICS_FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4631affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main() -> None:\n",
    "    # Paso 1: cargar y limpiar\n",
    "    df_train_raw, df_test_raw = load_raw_data()\n",
    "    df_train = clean_dataset(df_train_raw)\n",
    "    df_test = clean_dataset(df_test_raw)\n",
    "\n",
    "    # Paso 2: dividir en X/y\n",
    "    x_train, y_train = split_xy(df_train)\n",
    "    x_test, y_test = split_xy(df_test)\n",
    "\n",
    "    # Pasos 3 y 4: pipeline + GridSearchCV\n",
    "    model = build_model(x_train)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Paso 5: guardar modelo\n",
    "    save_model(model)\n",
    "\n",
    "    # Paso 6: métricas\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "\n",
    "    metrics_train = compute_classification_metrics(y_train, y_pred_train, \"train\")\n",
    "    metrics_test = compute_classification_metrics(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Paso 7: matrices de confusión\n",
    "    cm_train = compute_confusion_matrix_dict(y_train, y_pred_train, \"train\")\n",
    "    cm_test = compute_confusion_matrix_dict(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Guardar en metrics.json\n",
    "    save_metrics([metrics_train, metrics_test, cm_train, cm_test])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
